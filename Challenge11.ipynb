{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Q-Learning on FrozenLake ‚Äî CPU vs GPU Benchmark (PyTorch + Gym)\n",
        "\n",
        "This notebook benchmarks the performance of Q-learning on the FrozenLake environment using:\n",
        "\n",
        "- ‚úÖ **Pure Python (NumPy) on CPU**\n",
        "- ‚úÖ **PyTorch on GPU**\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Task Objective\n",
        "\n",
        "> Optimize the [FrozenLake Q-Learning code](https://github.com/ronanmmurphy/Q-Learning-Algorithm) for GPU using PyTorch and benchmark it against the CPU (NumPy) version.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ How to Run in Google Colab\n",
        "\n",
        "1. Open the notebook in [Google Colab](https://colab.research.google.com/)\n",
        "2. Set runtime type to **GPU**:  \n",
        "   `Runtime > Change runtime type > Hardware accelerator > GPU`\n",
        "3. Run all cells (`Runtime > Run all`)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Fixing Compatibility Issues\n",
        "\n",
        "To avoid this error:\n",
        "\n",
        "\n",
        "Downgrade NumPy at the top of the notebook using:\n",
        "\n",
        "```python\n",
        "!pip install numpy==1.23.5 --quiet\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # Force Colab restart\n"
      ],
      "metadata": {
        "id": "ECyHNJoyDW1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ronanmmurphy/Q-Learning-Algorithm.git\n",
        "%cd Q-Learning-Algorithm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp29lAga_gQQ",
        "outputId": "d116b520-7913-41d1-bac6-8e5369df226f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Q-Learning-Algorithm'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 16 (delta 3), reused 6 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (16/16), 32.26 KiB | 3.23 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "/content/Q-Learning-Algorithm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 --quiet\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "Ym0yQlBpAZka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gym\n",
        "import time\n",
        "\n",
        "# Create environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "# Get action and state sizes\n",
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 0.1\n",
        "episodes = 10000\n",
        "\n",
        "# Initialize Q-table using PyTorch on GPU\n",
        "Q = torch.zeros((state_size, action_size), dtype=torch.float32, device='cuda')\n",
        "\n",
        "def train_gpu():\n",
        "    for _ in range(episodes):\n",
        "        reset_result = env.reset()\n",
        "        state = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, device='cuda')\n",
        "            if torch.rand(1, device='cuda').item() < epsilon:\n",
        "                action = torch.randint(0, action_size, (1,), device='cuda').item()\n",
        "            else:\n",
        "                action = torch.argmax(Q[state_tensor]).item()\n",
        "\n",
        "            step_result = env.step(action)\n",
        "            if len(step_result) == 5:  # New API\n",
        "                new_state, reward, terminated, truncated, _ = step_result\n",
        "                done = terminated or truncated\n",
        "            else:  # Old API\n",
        "                new_state, reward, done, _ = step_result\n",
        "\n",
        "            new_state_tensor = torch.tensor(new_state, device='cuda')\n",
        "            Q[state_tensor, action] += alpha * (\n",
        "                reward + gamma * torch.max(Q[new_state_tensor]) - Q[state_tensor, action]\n",
        "            )\n",
        "            state = new_state\n",
        "\n",
        "start_gpu = time.time()\n",
        "train_gpu()\n",
        "end_gpu = time.time()\n",
        "\n",
        "print(f\"‚úÖ GPU Training Time: {end_gpu - start_gpu:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeeLEhUg_h0J",
        "outputId": "7cdab5b3-1417-48e3-cf17-b97e8fbb0175"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU Training Time: 286.9483 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Reset environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "# Reinitialize Q-table for CPU version\n",
        "Q_cpu = np.zeros((state_size, action_size))\n",
        "\n",
        "def train_cpu():\n",
        "    for _ in range(episodes):\n",
        "        reset_result = env.reset()\n",
        "        state = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.randint(action_size)\n",
        "            else:\n",
        "                action = np.argmax(Q_cpu[state])\n",
        "\n",
        "            step_result = env.step(action)\n",
        "            if len(step_result) == 5:\n",
        "                new_state, reward, terminated, truncated, _ = step_result\n",
        "                done = terminated or truncated\n",
        "            else:\n",
        "                new_state, reward, done, _ = step_result\n",
        "\n",
        "            Q_cpu[state, action] += alpha * (\n",
        "                reward + gamma * np.max(Q_cpu[new_state]) - Q_cpu[state, action]\n",
        "            )\n",
        "            state = new_state\n",
        "\n",
        "start_cpu = time.time()\n",
        "train_cpu()\n",
        "end_cpu = time.time()\n",
        "\n",
        "print(f\"‚úÖ CPU Training Time: {end_cpu - start_cpu:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69fsMrtH_shP",
        "outputId": "07757050-9b60-4e12-dc6e-a1e0ebd06a09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CPU Training Time: 24.7515 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speedup = (end_cpu - start_cpu) / (end_gpu - start_gpu)\n",
        "print(f\"üöÄ Speed-up (CPU/GPU): {speedup:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O6rCq2PCjDF",
        "outputId": "1b871e47-b721-4a6b-bc7a-d0f314f872c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Speed-up (CPU/GPU): 0.09x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Benchmark Results\n",
        "\n",
        "We trained a Q-Learning agent on the `FrozenLake-v1` environment for **10,000 episodes**, comparing performance between a classic **CPU (NumPy)** implementation and a **GPU-accelerated (PyTorch)** version.\n",
        "\n",
        "### üîÅ Environment\n",
        "- Gym: `FrozenLake-v1` (`is_slippery=False`)\n",
        "- Training Episodes: `10,000`\n",
        "- Reward: Default Gym rewards (0 or 1)\n",
        "- Runtime: Google Colab (Tesla T4 GPU)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚è± Training Time Comparison\n",
        "\n",
        "| Method            | Training Time (s)      |\n",
        "|-------------------|------------------------|\n",
        "| **CPU (NumPy)**   | `24.7515` seconds      |\n",
        "| **GPU (PyTorch)** | `286.9483` seconds     |\n",
        "| **Speed-Up**      | `0.09√ó` slower on GPU  |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Observation\n",
        "\n",
        "While GPU acceleration **typically helps** with deep learning models or large-scale matrix operations, in this case:\n",
        "\n",
        "- The **Q-table is small** (16√ó4)\n",
        "- The **environment is lightweight**\n",
        "- Overhead from **GPU memory transfer** outweighs computation benefits\n",
        "\n",
        "So, the GPU version performed **slower** than CPU for this specific task.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Takeaway\n",
        "\n",
        "> For small-scale environments like FrozenLake, stick with CPU (NumPy).  \n",
        "> For larger state/action spaces or deep Q-learning (DQN), GPU acceleration will shine.\n"
      ],
      "metadata": {
        "id": "1CPa1U1qHBUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Although GPU acceleration did not yield performance gains for FrozenLake due to its small state/action space, this exercise demonstrated how to adapt and benchmark tabular Q-learning on GPU using PyTorch ‚Äî a skill essential for scaling to more complex RL environments.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUe6qcSyHq_U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i5UZh74KH4jt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}